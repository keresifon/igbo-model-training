# Igbo LLM Training - Complete Documentation
Complete guide for training a state-of-the-art Igbo-English translation model using AWS SageMaker and Mistral-7B.

**Project Status:** ‚úÖ **Production Training Completed** (January 2026)

---

## üìö Documentation Structure

### Getting Started
- **README.md** - Project overview, quick start, and repository structure
- **MEMORY_OPTIMIZATION_CHANGES.md** - Critical OOM fixes validated in production
- **LESSONS_LEARNED.md** - Production insights from 168-hour training run

### Step-by-Step Guides

#### 01. Setup (30-60 minutes)
**[docs/01-setup.md](docs/01-setup.md)** 
- AWS account creation
- AWS CLI installation
- Service quota requests (ml.g5.xlarge instances)
- IAM role configuration
- S3 bucket setup
- SageMaker Domain creation

#### 02. Data Preparation (2-3 hours)
**[docs/02-data-preparation.md](docs/02-data-preparation.md)**
- Downloading NLLB dataset (6.1M sentence pairs)
- Data verification and alignment checking
- Creating bidirectional training format (19.5M examples)
- Converting to JSONL format
- Uploading to S3 (~5GB)
- Storage cost optimization

#### 03. SageMaker Training (7 days)
**[docs/03-sagemaker-training.md](docs/03-sagemaker-training.md)**
- Training script setup
- Hyperparameter configuration
- Launching training jobs
- Checkpoint management (CRITICAL - see [LESSONS_LEARNED.md](docs/LESSONS_LEARNED.md))
- Memory optimization strategies

#### 04. Monitoring (Daily checks)
**[docs/04-monitoring.md](docs/04-monitoring.md)**
- SageMaker Console monitoring
- CloudWatch Logs analysis
- GPU/CPU metrics tracking
- Training loss visualization
- Real-time monitoring scripts
- Email alerts setup
- Performance benchmarks

#### 05. Deployment (1-2 days)
**[docs/05-deployment.md](docs/05-deployment.md)**
- Downloading trained model
- Local deployment with Ollama
- Model conversion (PyTorch ‚Üí GGUF)
- Quantization (14GB ‚Üí 4GB)
- Cloud deployment options
- Performance optimization

---

## üöÄ Quick Start

```bash
# 1. Clone repository
git clone https://github.com/keresifon/igbo-model-training.git
cd igbo-model-training

# 2. Install AWS CLI
pip install awscli
aws configure

# 3. Download NLLB data
wget https://object.pouta.csc.fi/OPUS-NLLB/v1/moses/en-ig.txt.zip
unzip en-ig.txt.zip

# 4. Prepare training data
python scripts/prepare_nllb_training.py \
    --en-file NLLB.en-ig.en \
    --ig-file NLLB.en-ig.ig \
    --output-dir processed-nllb

# 5. Upload to S3
aws s3 sync processed-nllb/ s3://your-bucket/datasets/nllb/

# 6. Launch training (SageMaker Studio notebook)
# Use train_igbo_model_FIXED.py for production-validated configuration
# See docs/03-sagemaker-training.md for complete code
```

---

## üí∞ Cost Breakdown (Production Validated)

| Phase | Duration | Cost | Notes |
|-------|----------|------|-------|
| Setup | 1 hour | $0 | One-time |
| Data prep | 2 hours | $0.12 | S3 storage |
| Training | 168 hours (7 days) | $237 | On-demand instances |
| S3 Storage | Ongoing | $0.50/month | Data + checkpoints |
| **Total First Run** | **~8 days** | **~$238** | Production cost |

**Note on Spot vs On-Demand:**
- Spot instances: $71 (if uninterrupted) - experienced capacity issues
- On-demand: $237 (guaranteed) - used for production reliability
- **Recommendation:** Use on-demand for 7+ day training jobs

---

## üéØ Project Goals

‚úÖ Train production-quality Igbo-English translation model  
‚úÖ Use cost-effective cloud infrastructure (AWS SageMaker)  
‚úÖ Achieve 90-95% GPU utilization  
‚úÖ Deploy locally on consumer hardware (16GB RAM)  
‚úÖ Preserve and promote Igbo language üá≥üá¨  
‚úÖ Document production ML operations learnings  

---

## üìä Dataset Statistics

- **Source:** OPUS NLLB corpus
- **Sentence pairs:** 6,145,395
- **Training examples:** 19,471,872 (bidirectional augmentation)
- **Languages:** English ‚Üî Igbo
- **File size:** ~5GB (JSONL)
- **Split:** 99.5% train, 0.5% validation

---

## ü§ñ Model Details

### Base Model
- **Model:** Mistral-7B-v0.1 (7 billion parameters)
- **Architecture:** Decoder-only transformer
- **Context length:** 8192 tokens (training uses 256)
- **Vocabulary:** 32,000 tokens

### Fine-tuning
- **Method:** LoRA (Low-Rank Adaptation)
- **Trainable parameters:** 6.8M (0.094% of total)
- **Target modules:** q_proj, v_proj (2 modules for memory efficiency)
- **LoRA rank (r):** 16
- **LoRA alpha:** 32
- **Training technique:** Instruction tuning
- **Optimization:** AdamW with gradient checkpointing

### Production Performance ‚úÖ
- **Training time:** 168 hours (7 days continuous)
- **GPU utilization:** 90-95% maintained
- **Throughput:** 11.56 iterations/second
- **Peak memory:** 23GB / 24GB VRAM
- **Final validation loss:** ~1.26 (strong convergence)
- **Checkpoints saved:** ~30 (every 5000 steps)
- **OOM errors:** 0 (zero across 168 hours)

---

## üõ†Ô∏è Technical Stack

### Cloud Infrastructure
- **Platform:** AWS SageMaker Training Jobs
- **Storage:** AWS S3 (data + model artifacts)
- **Monitoring:** AWS CloudWatch (logs + metrics)
- **Instance:** ml.g5.xlarge (NVIDIA A10G, 24GB VRAM)
- **Pricing:** On-demand ($1.41/hour) for reliability

### Machine Learning
- **Python:** 3.11
- **PyTorch:** 2.3
- **Transformers:** 4.46
- **PEFT:** LoRA implementation
- **Datasets:** HuggingFace datasets library

### Deployment
- **Ollama:** Local inference server
- **llama.cpp:** GGUF conversion
- **Quantization:** Q4_K_M, Q5_K_M (4-5GB models)

---

## üì± Use Cases

### 1. Language Learning App
- Real-time Igbo-English translation
- Pronunciation practice
- Vocabulary building
- Cultural context preservation

### 2. Educational Tools
- Teaching Igbo to children
- Homework assistance
- Interactive storytelling
- Language preservation initiatives

### 3. Communication
- Family conversations
- Cultural events
- Travel assistance
- Business translation

---

## üîß Key Features

‚úÖ **Bidirectional Translation**
- English ‚Üí Igbo
- Igbo ‚Üí English
- Preserves cultural context

‚úÖ **Instruction Following**
- Understands natural language prompts
- Supports Igbo instructions
- Flexible formatting

‚úÖ **Production Validated**
- 168 hours continuous operation
- Zero OOM errors
- 90-95% GPU utilization
- Complete checkpoint preservation

‚úÖ **Cost Optimized**
- Memory-efficient LoRA training
- Quantized inference (6-8GB RAM)
- Documented cost-reliability tradeoffs

‚úÖ **Production Ready**
- Checkpoint validation callbacks
- Defensive save strategies
- Comprehensive error handling
- Monitoring and alerting
- Complete documentation

---

## üéâ Production Training Results (January 2026)

### Training Run Details
- **Job:** igbo-nllb-fixed-2026-01-24-02-17-36-460
- **Status:** ‚úÖ Successfully completed
- **Duration:** 168 hours (7 days continuous)
- **Instance:** ml.g5.xlarge (24GB VRAM)

### Key Metrics
- **GPU Utilization:** 90-95% maintained throughout
- **Peak Memory:** 23GB / 24GB (4% headroom)
- **Throughput:** 11.56 iterations/second
- **Training Steps:** ~152,000 steps completed
- **Epochs:** 2.86 / 3.0 completed
- **Checkpoints:** ~30 saved (every 5000 steps)

### Configuration Used
```python
# Memory-optimized configuration
per_device_train_batch_size = 1
gradient_accumulation_steps = 16
max_length = 256  # Reduced from 512
fp16 = True
gradient_checkpointing = True
lora_r = 16
target_modules = ["q_proj", "v_proj"]  # 2 modules only
```

### Cost
- **Total:** $237 ($1.41/hour √ó 168 hours)
- **On-demand reliability:** Worth the premium for 7-day jobs

---

## üö® Important Notes

### Training Considerations

**Memory Requirements**
- ‚úÖ ml.g5.xlarge (24GB VRAM) - VALIDATED
- ‚úÖ batch_size=1, gradient_accumulation=16
- ‚úÖ max_length=256 (sufficient for 95% of Igbo sentences)
- ‚úÖ Gradient checkpointing enabled
- ‚úÖ 2 LoRA target modules (q_proj, v_proj only)

**Checkpoint Strategy (CRITICAL)**
- ‚úÖ Set `save_total_limit=None` to keep ALL checkpoints
- ‚úÖ Save every 5000 steps (validated frequency)
- ‚úÖ Add checkpoint validation callbacks
- ‚úÖ Test checkpoint saving in first 90 minutes
- ‚ö†Ô∏è  **See LESSONS_LEARNED.md for checkpoint configuration details**

**Instance Selection**
- **7+ day jobs:** Use on-demand (guaranteed completion)
- **<3 day jobs:** Spot instances acceptable
- **First run:** Initial spot attempt failed after 41 hours
- **Production:** Switched to on-demand for reliability

### Deployment Considerations

**Local Inference (16GB RAM)**
- Use Q4_K_M or Q5_K_M quantization
- Expected RAM: 6-8GB
- Speed: 20-30 tokens/second
- Works great for personal use

**Cloud Deployment**
- SageMaker Endpoint: $1,016/month (24/7)
- Lambda: $10-50/month (sporadic use)
- Choose based on usage volume

---

## üìñ Troubleshooting Guide

### Common Issues

#### 1. Out of Memory (OOM)
- **Solution:** See MEMORY_OPTIMIZATION_CHANGES.md
- **Key fixes:** batch_size=1, max_length=256, gradient checkpointing
- **Validated:** Zero OOM errors in 168-hour production run

#### 2. Checkpoint Configuration Issues
- **Problem:** Only first checkpoint preserved
- **Root cause:** save_total_limit deleting old checkpoints
- **Solution:** Set save_total_limit=None
- **Details:** See LESSONS_LEARNED.md

#### 3. Spot Instance Interruptions
- **Problem:** Capacity interruptions after 41 hours
- **Solution:** Use on-demand for 7+ day jobs
- **Cost:** Accept $237 vs $71 for guaranteed completion

#### 4. Training Stuck/Slow
- Check CloudWatch logs for errors
- Verify S3 data accessibility
- Monitor GPU utilization (should be 90-95%)
- Confirm checkpoint saving every 5000 steps

---

## üéì Learning Resources

### AWS
- [SageMaker Training Documentation](https://docs.aws.amazon.com/sagemaker/)
- [Spot Instance Best Practices](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html)
- [Service Quotas Management](https://docs.aws.amazon.com/servicequotas/)

### Machine Learning
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [Mistral-7B Model](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- [PEFT Documentation](https://huggingface.co/docs/peft)

### Deployment
- [Ollama](https://ollama.ai)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [GGUF Format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)

---

## ü§ù Contributing

Contributions welcome! Areas for improvement:

### Model Evaluation
- BLEU, METEOR, chrF scores
- Human evaluation
- Test set creation
- Quality benchmarking

### Data Augmentation
- Back-translation
- Synthetic data generation
- Domain-specific corpora
- Dialect coverage

### Deployment Options
- Docker containers
- Kubernetes deployment
- Edge device optimization
- Mobile integration

### Documentation
- Video tutorials
- More translation examples
- Quality comparisons
- Best practices guide

---

## üìß Support

- **Issues:** [Open GitHub issue](https://github.com/keresifon/igbo-model-training/issues)
- **Discussions:** [GitHub Discussions](https://github.com/keresifon/igbo-model-training/discussions)
- **Repository:** https://github.com/keresifon/igbo-model-training

---

## üìÑ License

MIT License - See LICENSE file

---

## üôè Acknowledgments

- **Meta AI** for the NLLB dataset
- **Mistral AI** for the base model
- **AWS** for SageMaker infrastructure
- **OPUS** for dataset hosting
- **Igbo language community** for inspiration and cultural preservation

---

## üéØ Project Status

‚úÖ Data preparation complete  
‚úÖ Training script optimized and validated  
‚úÖ Documentation written  
‚úÖ **Production training completed (168 hours)**  
‚úÖ **Memory optimization validated**  
‚úÖ **Checkpoint strategy proven**  
‚è≥ Model evaluation (in progress)  
‚è≥ Production deployment (planned)  

---

## üìÖ Timeline

| Date | Milestone |
|------|-----------|
| January 6, 2026 | Project started, memory optimizations implemented |
| January 16, 2026 | First training run (checkpoint config discovered) |
| January 24, 2026 | Fixed training launched |
| January 31, 2026 | ‚úÖ **Training completed successfully (168 hours)** |
| February 2026 | Model evaluation and deployment |

**Status:** ‚úÖ **Production training completed successfully!**

---

## üåü Key Achievements

‚úÖ **168 hours continuous training** with zero interruptions  
‚úÖ **90-95% GPU utilization** maintained throughout  
‚úÖ **Zero OOM errors** across entire training run  
‚úÖ **~30 checkpoints preserved** with defensive strategy  
‚úÖ **Production ML operations** experience documented  
‚úÖ **Cost-reliability tradeoffs** analyzed and documented  

**Result:** Production-validated ML training pipeline ready for replication! üöÄüá≥üá¨

---

**Repository:** https://github.com/keresifon/igbo-model-training  
**Last Updated:** January 31, 2026