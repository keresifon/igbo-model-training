# Igbo LLM Training - Complete Documentation

Complete guide for training a state-of-the-art Igbo-English translation model using AWS SageMaker and Mistral-7B.

## üìö Documentation Structure

### Getting Started
1. **[README.md](README.md)** - Project overview, quick start, and repository structure
2. **[MEMORY_OPTIMIZATION_CHANGES.md](MEMORY_OPTIMIZATION_CHANGES.md)** - Critical fixes for OOM errors during training

### Step-by-Step Guides

#### 01. Setup (30-60 minutes)
**[docs/01-setup.md](docs/01-setup.md)**
- AWS account creation
- AWS CLI installation
- Service quota requests (ml.g5.xlarge spot instances)
- IAM role configuration
- S3 bucket setups
- SageMaker Domain creation

#### 02. Data Preparation (2-3 hours)
**[docs/02-data-preparation.md](docs/02-data-preparation.md)**
- Downloading NLLB dataset (6.1M sentence pairs)
- Data verification and alignment checking
- Creating 4 training formats per pair (19.4M examples)
- Converting to JSONL format
- Uploading to S3 (~5GB)
- Storage cost optimization

#### 03. SageMaker Training (7-8 days)
**[docs/03-sagemaker-training.md](docs/03-sagemaker-training.md)**
- Training script setup
- Hyperparameter configuration
- Launching training jobs
- Checkpoint management
- Troubleshooting OOM errors

#### 04. Monitoring (Daily checks)
**[docs/04-monitoring.md](docs/04-monitoring.md)**
- SageMaker Console monitoring
- CloudWatch Logs analysis
- GPU/CPU metrics tracking
- Training loss visualization
- Real-time monitoring scripts
- Email alerts setup
- Performance benchmarks

#### 05. Deployment (1-2 days)
**[docs/05-deployment.md](docs/05-deployment.md)**
- Downloading trained model
- Local deployment with Ollama
- Model conversion (PyTorch ‚Üí GGUF)
- Quantization (14GB ‚Üí 4GB)
- Cloud deployment options
- Performance optimization

---

## üöÄ Quick Start

```bash
# 1. Clone repository
git clone https://github.com/your-username/igbo-llm-training.git
cd igbo-llm-training

# 2. Install AWS CLI
pip install awscli
aws configure

# 3. Download NLLB data
wget https://object.pouta.csc.fi/OPUS-NLLB/v1/moses/en-ig.txt.zip
unzip en-ig.txt.zip

# 4. Prepare training data
python scripts/prepare_nllb_training.py \
    --en-file NLLB.en-ig.en \
    --ig-file NLLB.en-ig.ig \
    --output-dir processed-nllb

# 5. Upload to S3
aws s3 sync processed-nllb/ s3://your-bucket/datasets/nllb/

# 6. Launch training (SageMaker Studio notebook)
# See docs/03-sagemaker-training.md for complete code
```

---

## üí∞ Cost Breakdown

| Phase | Duration | Cost |
|-------|----------|------|
| Setup | 1 hour | $0 |
| Data prep | 2 hours | $0.12 (S3 storage) |
| Training | 7-8 days | $70-85 (spot instances) |
| Deployment | N/A | $0 (local) or varies (cloud) |
| **Total** | **~8 days** | **~$71-86** |

---

## üéØ Project Goals

1. ‚úÖ Train production-quality Igbo-English translation model
2. ‚úÖ Use cost-effective cloud infrastructure (AWS spot instances)
3. ‚úÖ Deploy locally on Mac Mini (16GB RAM)
4. ‚úÖ Integrate into iOS app for language learning
5. ‚úÖ Preserve and promote Igbo language üá≥üá¨

---

## üìä Dataset Statistics

- **Source:** OPUS NLLB corpus
- **Sentence pairs:** 6,145,395
- **Training examples:** 19,471,872 (4 formats per pair)
- **Languages:** English ‚Üî Igbo
- **File size:** ~5GB (JSONL)
- **Split:** 99.5% train, 0.5% validation

---

## ü§ñ Model Details

**Base Model:**
- Mistral-7B-v0.1 (7 billion parameters)
- Decoder-only transformer
- Context length: 8192 tokens
- Vocabulary: 32,000 tokens

**Fine-tuning:**
- Method: LoRA (Low-Rank Adaptation)
- Trainable parameters: ~40M (0.5% of total)
- Training technique: Instruction tuning
- Optimization: AdamW with gradient checkpointing

**Performance:**
- Training time: 165-200 hours
- Final loss: ~1.2 (expected)
- BLEU score: TBD after evaluation
- Inference speed: 20-30 tokens/second (local, quantized)

---

## üõ†Ô∏è Technical Stack

**Cloud Infrastructure:**
- AWS SageMaker Training
- AWS S3 (data storage)
- AWS CloudWatch (monitoring)
- Instance: ml.g5.xlarge (NVIDIA A10G, 24GB VRAM)
- Spot instances (70% cost savings)

**Machine Learning:**
- Python 3.11
- PyTorch 2.3
- Transformers 4.46
- PEFT (LoRA implementation)
- Datasets library

**Deployment:**
- Ollama (local inference)
- llama.cpp (GGUF conversion)
- GGUF quantization (Q4_K_M, Q5_K_M)
- Swift (iOS integration)

---

## üì± Use Cases

### 1. Language Learning App
- Real-time Igbo-English translation
- Pronunciation practice
- Vocabulary building
- Cultural context

### 2. Educational Tools
- Teaching Igbo to children
- Homework assistance
- Interactive storytelling
- Language preservation

### 3. Communication
- Family conversations
- Cultural events
- Travel assistance
- Business translation

---

## üîß Key Features

‚úÖ **Bidirectional Translation**
- English ‚Üí Igbo
- Igbo ‚Üí English
- Preserves cultural context

‚úÖ **Instruction Following**
- Understands natural language prompts
- Supports Igbo instructions
- Flexible formatting

‚úÖ **Cost Optimized**
- Spot instances (70% savings)
- Efficient LoRA training
- Quantized inference

‚úÖ **Production Ready**
- Checkpoint resuming
- Error handling
- Monitoring tools
- Deployment guides

---

## üö® Important Notes

### Training Considerations

1. **Memory Requirements**
   - ml.g5.xlarge minimum (24GB VRAM)
   - Removed `prepare_model_for_kbit_training` (caused OOM)
   - Gradient checkpointing enabled
   - Reduced LoRA modules (2 instead of 4)

2. **Spot Instance Behavior**
   - May be interrupted (automatic resume)
   - Checkpoints saved every 1000 steps
   - Expected 0-2 interruptions over 7 days
   - No action required from you

3. **Training Time**
   - Expected: 165-200 hours (7-8 days)
   - Varies with spot availability
   - First epoch: ~55-65 hours
   - Progress visible in logs

### Deployment Considerations

1. **Local Inference (Mac Mini 16GB)**
   - Use Q4_K_M or Q5_K_M quantization
   - Expected RAM: 6-8GB
   - Speed: 20-30 tokens/second
   - Works great for personal use

2. **iOS Deployment**
   - Requires iPhone 12+ (4GB+ RAM)
   - Model size: ~4GB (quantized)
   - On-device or server-based options
   - Speed: 10-20 tokens/second

3. **Cloud Deployment**
   - SageMaker: $1,016/month (24/7)
   - Lambda: $10-50/month (sporadic)
   - Choose based on usage volume

---

## üìñ Troubleshooting Guide

### Common Issues

**1. Out of Memory (OOM)**
- See: [MEMORY_OPTIMIZATION_CHANGES.md](MEMORY_OPTIMIZATION_CHANGES.md)
- Solution: Removed `prepare_model_for_kbit_training`
- Fallback: Use ml.g5.2xlarge ($0.84/hour spot)

**2. Spot Instance Unavailable**
- Wait 30-60 minutes and retry
- Or use on-demand: `use_spot_instances=False`

**3. Training Stuck**
- Check CloudWatch logs for errors
- Verify S3 data accessibility
- Monitor GPU utilization

**4. Poor Translation Quality**
- Ensure training completed (check loss curve)
- Try better prompts
- Use less aggressive quantization

---

## üéì Learning Resources

### AWS
- [SageMaker Training Docs](https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html)
- [Spot Instance Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)

### Machine Learning
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/)
- [PEFT Documentation](https://huggingface.co/docs/peft)

### Deployment
- [Ollama](https://ollama.com/)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [GGUF Format](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)

---

## ü§ù Contributing

Contributions welcome! Areas for improvement:

1. **Model Evaluation**
   - BLEU, METEOR, chrF scores
   - Human evaluation
   - Test set creation

2. **Data Augmentation**
   - Back-translation
   - Synthetic data generation
   - Domain-specific corpora

3. **Deployment Options**
   - Docker containers
   - Kubernetes deployment
   - Edge device optimization

4. **Documentation**
   - Video tutorials
   - More examples
   - Translation quality comparisons

---

## üìß Support

- **Issues:** Open GitHub issue
- **Questions:** Discussions tab
- **Email:** your-email@example.com

---

## üìÑ License

MIT License - See LICENSE file

---

## üôè Acknowledgments

- **NLLB Team** for the dataset
- **Mistral AI** for the base model
- **Anthropic** for Claude (documentation assistance)
- **Igbo language community** for inspiration

---

## üéØ Project Status

- [x] Data preparation complete
- [x] Training script optimized
- [x] Documentation written
- [ ] Training in progress (7-8 days remaining)
- [ ] Model evaluation (pending)
- [ ] iOS app integration (pending)
- [ ] Production deployment (pending)

---

## üìÖ Timeline

| Date | Milestone |
|------|-----------|
| Jan 6, 2026 | Project started, data prepared |
| Jan 6, 2026 | Training launched (7-8 days) |
| Jan 13-14, 2026 | Training completes |
| Jan 15, 2026 | Model evaluation |
| Jan 16-17, 2026 | Local deployment setup |
| Jan 18+, 2026 | iOS app integration |

---

**Status:** ‚è≥ Training in progress...

**Check back in 7-8 days for the completed model!** üöÄüá≥üá¨

---

*Last updated: January 6, 2026*
